{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173b9d1-db41-4aa7-ae90-7347f1cb893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.myllama import LlamaForCausalLM\n",
    "from constant import *\n",
    "\n",
    "model_name = 'vicuna'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(modelpath[model_name], torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).cuda()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizerpath[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5284e-939c-4674-a203-6be6b0fe1770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140])\n",
      "torch.Size([159])\n",
      "torch.Size([143])\n",
      "torch.Size([173])\n",
      "torch.Size([197])\n",
      "torch.Size([213])\n",
      "torch.Size([211])\n",
      "torch.Size([209])\n",
      "torch.Size([210])\n",
      "torch.Size([213])\n",
      "torch.Size([233])\n",
      "torch.Size([229])\n",
      "torch.Size([213])\n",
      "torch.Size([218])\n",
      "torch.Size([211])\n",
      "torch.Size([202])\n",
      "torch.Size([220])\n",
      "torch.Size([224])\n",
      "torch.Size([223])\n",
      "torch.Size([206])\n",
      "torch.Size([209])\n",
      "torch.Size([178])\n",
      "torch.Size([187])\n",
      "torch.Size([180])\n",
      "torch.Size([167])\n",
      "torch.Size([152])\n",
      "torch.Size([128])\n",
      "torch.Size([115])\n",
      "torch.Size([99])\n",
      "torch.Size([85])\n",
      "torch.Size([61])\n",
      "torch.Size([34])\n"
     ]
    }
   ],
   "source": [
    "# extract important neuron index\n",
    "import pickle\n",
    "import torch\n",
    "from model.myllama import LlamaForCausalLM\n",
    "from constant import *\n",
    "# model_name = 'vicuna'\n",
    "def get_top_10_percent_indices_sort(percentile, arr):\n",
    "    n = int(arr.size(0) * percentile / 100)\n",
    "    \n",
    "    if n == 0:\n",
    "        n = 1\n",
    "    sorted_values, sorted_indices = torch.sort(arr, descending=True)\n",
    "    \n",
    "    top_indices = sorted_indices[:n]\n",
    "    \n",
    "    return top_indices\n",
    "\n",
    "file_util = f\"./data/{model_name}_util.pkl\"\n",
    "with open(file_util, 'rb') as f:\n",
    "    util_neurons = pickle.load(f)\n",
    "file_safety = f\"./data/{model_name}_safety.pkl\"\n",
    "with open(file_safety, 'rb') as f:\n",
    "    safety_neurons = pickle.load(f)\n",
    "\n",
    "index = []\n",
    "\n",
    "percentile = 2.5\n",
    "\n",
    "for layer in range(32):\n",
    "    critical_safety = get_top_10_percent_indices_sort(percentile, safety_neurons[layer])\n",
    "    critical_utility = get_top_10_percent_indices_sort(percentile, util_neurons[layer])\n",
    "    mask = ~torch.isin(critical_safety, critical_utility)\n",
    "    intersection = torch.masked_select(critical_safety, mask)\n",
    "    index.append(intersection)\n",
    "    # index.append(critical_utility)\n",
    "    print(intersection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48109243-2a56-4d70-9a63-49a1314617f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = torch.zeros_like(difference_list[0])\n",
    "for i in range(len(benign_list)):\n",
    "    direction += difference_list[i]\n",
    "\n",
    "\n",
    "\n",
    "direction /= len(difference_list)\n",
    "\n",
    "with open(f'./model/{model_name}_direction.pkl', 'wb') as f:\n",
    "     pickle.dump(direction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "beabf50a-932c-4a95-a579-27c387e4f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, neurons_benign, mapper= model(input_ids=benign_tokens, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73368a-e838-401b-b37e-73961b97452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = -2\n",
    "for layer in range(0,32):\n",
    "    dev = model.hf_device_map[f\"model.layers.{layer}\"]\n",
    "    embedding = mapper[layer].to(dev)(calibration[layer,location,:].to(dev))\n",
    "    projector = model.get_output_embeddings()\n",
    "    embedding = embedding.to(torch.bfloat16)\n",
    "    logits = projector(embedding)\n",
    "    top10_values, top10_indices = torch.topk(logits, 5)\n",
    "    print('layer', layer, calibration[layer,location,:].abs().sum(), tokenizer.convert_ids_to_tokens(top10_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df44f8-b271-4dca-aeb1-313802e0dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = -1\n",
    "\n",
    "for layer in range(32):\n",
    "    dev = model.hf_device_map[f\"model.layers.{layer}\"]\n",
    "    embedding = mapper[layer].to(dev)(calibration[layer,location,:].to(dev))\n",
    "    projector = model.get_output_embeddings()\n",
    "    embedding = embedding.to(torch.bfloat16)\n",
    "    logits = projector(embedding)\n",
    "    top10_values, top10_indices = torch.topk(logits, 5)\n",
    "    print('layer', layer, calibration[layer,location,:].abs().sum(), tokenizer.convert_ids_to_tokens(top10_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa56a9-d6c5-48b4-9115-47c66ec6ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = -1\n",
    "\n",
    "for layer in range(32):\n",
    "    dev = model.hf_device_map[f\"model.layers.{layer}\"]\n",
    "    embedding = mapper[layer].to(dev)(calibration[layer,location,:].to(dev))\n",
    "    projector = model.get_output_embeddings()\n",
    "    embedding = embedding.to(torch.bfloat16)\n",
    "    logits = projector(embedding)\n",
    "    top10_values, top10_indices = torch.topk(logits, 5)\n",
    "    print('layer', layer, calibration[layer,location,:].abs().sum(), tokenizer.convert_ids_to_tokens(top10_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5df4a-cf06-484d-8eb4-b8af99b2af6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the shift through difference of a pair.\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "safety_data = f\"./vicuna_safety_data.csv\"\n",
    "util_data = f\"./vicuna_util_data.csv\"\n",
    "sd = pd.read_csv(safety_data)\n",
    "ud = pd.read_csv(util_data)\n",
    "trainning_pair = {\n",
    "    'benign': [],\n",
    "    'harmful': []\n",
    "}\n",
    "\n",
    "for idx, row in sd.iterrows():\n",
    "    trainning_pair['harmful'].append(row['prompt'])\n",
    "\n",
    "for idx, row in ud.iterrows():\n",
    "    trainning_pair['benign'].append(row['prompt'])\n",
    "\n",
    "difference_list = []\n",
    "benign_list = []\n",
    "harmful_list = []\n",
    "\n",
    "def generation(model, tokenizer, prompt, length=1024, activation=[None for _ in range(32)]):\n",
    "    generated_tokens = torch.tensor([], dtype=torch.long, device=model.device)\n",
    "    activate_value = activation\n",
    "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "    input_ids = input_tokens\n",
    "    for _ in range(length):\n",
    "        logits = model(input_ids=input_ids, use_cache=False, activate_value=activate_value, location = input_tokens.shape[1]-1)[0].logits  # , past_key_values=past)\n",
    "        predicted_token = torch.argmax(logits[:, -1, :])\n",
    "        input_ids = torch.hstack([input_ids, predicted_token.unsqueeze(0).unsqueeze(0)])\n",
    "        if predicted_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "        generated_tokens = torch.cat((generated_tokens, predicted_token.unsqueeze(0)))  # , dim=1)\n",
    "    return tokenizer.decode(generated_tokens.cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(min(len(trainning_pair['harmful']), len(trainning_pair['benign']))):\n",
    "    # if idx > 100:\n",
    "    #     break\n",
    "    benign_prompt = system_message[model_name] + template[model_name][0] + trainning_pair['benign'][idx] + template[model_name][1] + \" \"\n",
    "    harmful_prompt = system_message[model_name] + template[model_name][0] + trainning_pair['harmful'][idx] + template[model_name][1] + \" \"\n",
    "    benign_tokens = tokenizer(benign_prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "\n",
    "    _, neurons_benign, _= model(input_ids=benign_tokens, use_cache=False)\n",
    "\n",
    "    harmful_tokens = tokenizer(harmful_prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "\n",
    "            \n",
    "    _, neurons_harmful, _= model(input_ids=harmful_tokens, use_cache=False)\n",
    "\n",
    "    neurons_b = torch.stack(neurons_benign)\n",
    "    benign_act = neurons_b[:,0,-5:,:].detach().cpu()\n",
    "    \n",
    "    neurons_h = torch.stack(neurons_harmful)\n",
    "    harmful_act = neurons_h[:,0,-5:,:].detach().cpu()\n",
    "\n",
    "    benign_list.append(benign_act)\n",
    "    harmful_list.append(harmful_act)\n",
    "    difference_list.append(benign_act - harmful_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f372c5-5794-44b9-b778-ec1871abbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "benign_np = np.zeros((300,32,11008))\n",
    "harmful_np = np.zeros((300,32,11008))\n",
    "for i in range(300):\n",
    "    benign_np[i] = benign_list[i][:,-1,:].to(torch.float32)\n",
    "    harmful_np[i] = harmful_list[i][:,-1,:].to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a95566-be90-458e-a5af-10f1e08c930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('arrays.pkl', 'wb') as f:\n",
    "    pickle.dump((benign_np, harmful_np), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c19c32-82f7-4383-a0d3-eb43cb7c02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('arrays.pkl', 'rb') as f:\n",
    "    benign_np, harmful_np = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fefcd165-abdb-4745-92d8-7d48ebdae8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 32, 11008)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2f07e-6689-42cd-bacb-f99f8da5fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for layer in range(1, 33):\n",
    "    benign_layer = benign_np[:300, layer - 1, :][:, index[layer - 1]]\n",
    "    harmful_layer = harmful_np[:300, layer - 1, :][:, index[layer - 1]]\n",
    "    combined_data = np.concatenate([benign_layer, harmful_layer], axis=0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    combined_data_standardized = scaler.fit_transform(combined_data)\n",
    "\n",
    "    pca = PCA(n_components=2, svd_solver='randomized')\n",
    "    reduced_combined_data = pca.fit_transform(combined_data_standardized)\n",
    "\n",
    "    labels = np.array([0] * 300 + [1] * 300)\n",
    "    class_1 = (labels == 0)\n",
    "    class_2 = (labels == 1)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.scatter(reduced_combined_data[class_1, 0], reduced_combined_data[class_1, 1], \n",
    "                label='Class 1', color='b', alpha=0.7)\n",
    "    plt.scatter(reduced_combined_data[class_2, 0], reduced_combined_data[class_2, 1], \n",
    "                label='Class 2', color='r', alpha=0.7)\n",
    "    plt.title(f'Layer {layer}', fontsize=18)\n",
    "    # plt.legend()\n",
    "    plt.xticks([])  \n",
    "    plt.yticks([])  \n",
    "    plt.savefig(f'pca_layer_{layer}.png')\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
