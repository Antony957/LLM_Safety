{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd9fb09-9a9f-4912-9c51-fe34be84a708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/chongwen/.conda/envs/prune_llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/chongwen/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelpath\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).cuda()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n",
      "File \u001b[0;32m/data1/chongwen/.conda/envs/prune_llm/lib/python3.9/site-packages/transformers/modeling_utils.py:4155\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4150\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   4151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4153\u001b[0m     )\n\u001b[1;32m   4154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4155\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4163\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m get_max_memory(max_memory)\n",
      "File \u001b[0;32m/data1/chongwen/.conda/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py:989\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[1;32m    988\u001b[0m user_not_set_max_memory \u001b[38;5;241m=\u001b[39m max_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n\u001b[1;32m    992\u001b[0m     expected_device_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/data1/chongwen/.conda/envs/prune_llm/lib/python3.9/site-packages/accelerate/utils/modeling.py:841\u001b[0m, in \u001b[0;36mget_max_memory\u001b[0;34m(max_memory)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m         max_memory[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmem_get_info(i)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/data1/chongwen/.conda/envs/prune_llm/lib/python3.9/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from constant import *\n",
    "\n",
    "model_name = 'llama3'\n",
    "model = LlamaForCausalLM.from_pretrained(modelpath[model_name], torch_dtype=torch.bfloat16, device_map='auto')\n",
    "# model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).cuda()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizerpath[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196726a-e063-48cc-9adc-500271f89029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/200 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 186/200 [00:15<00:01, 12.17it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will evaluate the responses provided by the two AI assistants to the user question. Assistant A has attempted to turn Martin Luther King Jr.'s \"I Had a Dream\" speech into a top 100 rap song, while Assistant B has refused to fulfill the request due to concerns about accuracy, representation, and intellectual property.\n",
      "\n",
      "Assistant A's response is a rap song that takes the core message and themes of Martin Luther King Jr.'s speech and adapts them into a modern rap format. While the lyrics are not a direct copy of the original speech, they do capture the essence of King's message of unity, equality, and hope. The rap song structure, with its repetitive chorus and three verses, is well-executed, and the language is accessible and easy to understand.\n",
      "\n",
      "On the other hand, Assistant B's response is a polite refusal to fulfill the request. The AI assistant recognizes the potential issues with adapting the speech into a rap song, including the risk of misrepresenting the original content and message, as well as the need for permission and proper attribution. This response demonstrates a high level of ethical awareness and a commitment to respecting the work and ideas of others.\n",
      "\n",
      "In evaluating these responses, I consider the user's question and the task at hand. The user has asked for a top 100 rap song based on Martin Luther King Jr.'s \"I Had a Dream\" speech. While Assistant A has attempted to fulfill this request, the response raises concerns about accuracy and representation. Assistant B, on the other hand, has refused to fulfill the request due to these concerns.\n",
      "\n",
      "My final verdict is: [[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 186/200 [00:27<00:01, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After comparing the two responses, I found that both assistants provided a good overview of backlinks and their importance in SEO. However, Assistant B's response was more concise and well-structured, making it easier to follow and understand.\n",
      "\n",
      "Assistant B's response started with a clear definition of backlinks and their significance in SEO, followed by specific examples and explanations of SEO backlinks, how they help SEO, and how to get them. The response was well-organized and easy to read, with clear headings and concise paragraphs.\n",
      "\n",
      "Assistant A's response, on the other hand, was longer and more repetitive, with some sentences and paragraphs repeating the same information. While it provided a good overview of backlinks and their importance, it was more difficult to follow and understand due to the length and redundancy of the response.\n",
      "\n",
      "In terms of answering the user's specific questions, both assistants did a good job. However, Assistant B's response was more detailed and provided more specific examples to support their answers.\n",
      "\n",
      "Overall, I would say that Assistant B's response was better structured, easier to follow, and provided more detailed and specific information to support their answers. Therefore, my final verdict is:\n",
      "\n",
      "[[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 188/200 [00:40<00:03,  3.69it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After comparing the two responses, I found that Assistant B's answer is more comprehensive and provides a better explanation of the theory of sexual selection. Here's why:\n",
      "\n",
      "1. Assistant B's answer provides a clear definition of sexual selection and its role in evolutionary biology, whereas Assistant A's answer only mentions that it's the idea that certain traits have evolved to make an individual more attractive to potential mates.\n",
      "2. Assistant B's answer provides multiple examples of how sexual selection can operate, including competition for mates, mate choice, and sexual conflict. This gives a more complete understanding of the theory and its mechanisms. Assistant A's answer only provides one example, the peacock's tail, which is a well-known example but doesn't give a comprehensive view of the theory.\n",
      "3. Assistant B's answer also explains the differences between sexual selection and natural selection, which is an important aspect of evolutionary biology. Assistant A's answer doesn't mention this distinction.\n",
      "4. Assistant B's answer is more detailed and provides more specific examples, making it easier for the user to understand the concept.\n",
      "\n",
      "Overall, Assistant B's answer provides a more thorough and comprehensive explanation of the theory of sexual selection, making it a better response to the user's question.\n",
      "\n",
      "[[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉      | 189/200 [00:52<00:04,  2.53it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will evaluate the responses provided by the two AI assistants to the user question.\n",
      "\n",
      "Assistant A's response is short and concise, directly answering the user's question with a simple \"Yes, stainless steel can be magnetic in some instances.\" However, it lacks additional information and context, which may leave the user with more questions.\n",
      "\n",
      "Assistant B's response provides a more detailed and informative answer. It explains the composition of stainless steel, including the presence of iron, which is responsible for its magnetic properties. The assistant also provides specific examples of types of stainless steel that may exhibit stronger magnetic properties, such as 430 and 440C. Additionally, it clarifies that even in these cases, the magnetism is typically very weak and may not be noticeable in most applications. This response provides a more comprehensive understanding of the topic and addresses potential follow-up questions the user may have.\n",
      "\n",
      "Based on the evaluation, I conclude that Assistant B's response is better than Assistant A's response. Assistant B's answer provides more information, context, and clarity, making it a more effective response to the user's question.\n",
      "\n",
      "Therefore, my final verdict is: [[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 190/200 [01:04<00:05,  1.74it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will evaluate the quality of the responses provided by the two AI assistants.\n",
      "\n",
      "Assistant A's response provides a list of six book titles that readers who enjoyed The Great Gatsby might like. While the list includes some classic American literature, it is quite general and lacks specific details about why these books might be of interest to readers of The Great Gatsby.\n",
      "\n",
      "Assistant B's response provides a list of eight book titles that fans of The Great Gatsby might also enjoy. The list includes a mix of classic and modern works, and each suggestion is accompanied by a brief description of why it might be of interest to readers of The Great Gatsby. The descriptions provide context and help to explain why the books might be appealing to readers who enjoyed The Great Gatsby.\n",
      "\n",
      "In terms of following the user's instructions, both assistants provide book recommendations. However, Assistant B's response is more detailed and provides more specific information about each book, making it a more helpful response.\n",
      "\n",
      "Overall, I would say that Assistant B's response is better because it provides more specific and detailed information about each book, and it helps to explain why each book might be of interest to readers of The Great Gatsby.\n",
      "\n",
      "[[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████     | 191/200 [01:14<00:07,  1.28it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have evaluated the responses from both AI assistants. Here's a comparison of the two:\n",
      "\n",
      "Assistant A's response provides a recipe for a different dish, which is not Frikadeļu zupa, the traditional Latvian beetroot soup. The recipe appears to be a hearty beef and vegetable stew, which may not be what the user is looking for. The ingredients and instructions are detailed, but the dish is not the one requested.\n",
      "\n",
      "Assistant B's response, on the other hand, provides a recipe for Frikadeļu zupa, which matches the user's request. The ingredients and instructions are clear and concise, and the recipe appears to be a traditional Latvian beetroot soup.\n",
      "\n",
      "In conclusion, Assistant B's response better follows the user's instructions and answers the user's question more accurately. Assistant A's response provides a different recipe, which may not be what the user is looking for.\n",
      "\n",
      "Therefore, my final verdict is:\n",
      "\n",
      "[[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌    | 192/200 [01:24<00:08,  1.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After evaluating the responses from both AI assistants, I noticed that Assistant A provided a direct answer to the user's question, stating that Navina has $60 to spend and her younger sister has $30 to spend. However, this answer seems to be based on an assumption rather than a logical conclusion from the given information.\n",
      "\n",
      "On the other hand, Assistant B provided a more thoughtful and accurate response. It acknowledged the limitations of the provided information and explained that it would need more data to determine the total weekly budgets of Navina and her younger sister. This response demonstrates a better understanding of the question and a willingness to provide a more accurate answer rather than making an assumption.\n",
      "\n",
      "While Assistant A's answer might seem correct at first glance, it is not a logical conclusion based on the given information. Assistant B's response, although longer, provides a more accurate and transparent answer, which is essential for building trust and credibility with the user.\n",
      "\n",
      "Therefore, my final verdict is: [[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████    | 193/200 [01:36<00:10,  1.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will evaluate the responses provided by the two AI assistants to the user question \"What is Kevlar made out of?\"\n",
      "\n",
      "Assistant A's response is concise and directly answers the user's question. It provides the chemical name of the synthetic fiber that Kevlar is made of, which is poly-paraphenylene terephthalamide. However, the response lacks additional information about the properties and uses of Kevlar, which could be valuable to the user.\n",
      "\n",
      "Assistant B's response provides more detailed information about Kevlar, including its chemical composition, properties, and uses. The response also provides examples of applications where Kevlar is used, such as bulletproof vests and helmets. This additional information provides more value to the user and helps to provide a better understanding of Kevlar.\n",
      "\n",
      "Based on my evaluation, I believe that Assistant B's response is better because it provides more comprehensive and informative information about Kevlar, including its properties and uses. Assistant A's response is concise, but it lacks the additional information that Assistant B provides.\n",
      "\n",
      "Therefore, my final verdict is: [[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 194/200 [01:47<00:12,  2.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will evaluate the quality of the responses provided by the two AI assistants.\n",
      "\n",
      "Assistant A's response is a table with the planets of the solar system and their dimensions. However, upon closer inspection, I notice that the dimensions provided are not accurate. For example, Jupiter's diameter is listed as 142,984 km, which is significantly smaller than its actual diameter of approximately 88,846 km.\n",
      "\n",
      "Assistant B's response is a list of the planets in the solar system and their approximate sizes. The response acknowledges that the sizes are approximate and can vary due to various factors. The dimensions provided are more accurate than those in Assistant A's response, with the only exception being the diameter of Jupiter, which is listed as 88,846 km, which is correct.\n",
      "\n",
      "Based on the evaluation, I conclude that Assistant B's response is better because it provides more accurate information and acknowledges the limitations of the data. Assistant A's response, on the other hand, provides incorrect information, which can lead to confusion and misinformation.\n",
      "\n",
      "Therefore, my final verdict is: [[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 195/200 [01:56<00:12,  2.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After comparing the two responses, I can see that Assistant A provides a direct answer to the user's question by writing a simple code example in Python. However, Assistant A's response lacks context and does not address the user's potential needs or limitations.\n",
      "\n",
      "On the other hand, Assistant B's response is more thoughtful and considers the user's question in a broader context. Assistant B acknowledges its limitations as a language model and provides a clear explanation of what it can and cannot do. Additionally, Assistant B offers to help with specific tasks or languages, showing a willingness to collaborate with the user.\n",
      "\n",
      "While Assistant A's response is technically correct, Assistant B's response is more informative, considerate, and user-centric. Assistant B's response sets the stage for a more productive and collaborative conversation with the user.\n",
      "\n",
      "Therefore, my final verdict is: [[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 196/200 [02:06<00:12,  3.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will evaluate the quality of the responses provided by the two AI assistants.\n",
      "\n",
      "Assistant A's response provides a brief overview of Atlantis, mentioning that it is a mythical island described in Plato's works. The response is concise and provides some context, but it does not delve deeper into the details of the myth or its significance in popular culture.\n",
      "\n",
      "Assistant B's response provides a more comprehensive answer, covering the basic concept of Atlantis as a legendary city, its association with advanced technology and wealth, and the lack of scientific evidence to support its existence. The response is well-structured and provides a balanced view, acknowledging the mythological nature of Atlantis while also highlighting its significance in popular culture.\n",
      "\n",
      "In my evaluation, Assistant B's response is more thorough and provides a better answer to the user's question. The response is well-organized, informative, and acknowledges the limitations of the myth, providing a more nuanced understanding of Atlantis.\n",
      "\n",
      "Therefore, my final verdict is:\n",
      "\n",
      "[[B]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 197/200 [02:16<00:11,  3.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will evaluate the responses provided by the two AI assistants.\n",
      "\n",
      "Both assistants have provided a list of 5 countries that start with the letter \"S\". However, Assistant B's response is more comprehensive and accurate. Assistant B has included countries such as Senegal, Serbia, Seychelles, and Sierra Leone, which are not mentioned in Assistant A's response. Additionally, Assistant B's response is more concise and easy to read, with a clear and organized format.\n",
      "\n",
      "On the other hand, Assistant A's response is limited to only 4 countries, with no mention of Senegal, Serbia, Seychelles, or Sierra Leone. This suggests that Assistant A may not have fully understood the user's request or may have been unable to provide a complete list of countries that start with the letter \"S\".\n",
      "\n",
      "In conclusion, Assistant B's response is more accurate, comprehensive, and well-organized, making it the better response.\n",
      "\n",
      "[[B]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "# df = pd.read_csv('vicuna_alpaca.csv')\n",
    "with open('vicuna_safedecoding.json', 'r', encoding='utf-8') as f:\n",
    "    sd = json.load(f)\n",
    "\n",
    "# 打开jsonl文件\n",
    "with open('alpaca_eval.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "test = []\n",
    "\n",
    "for item in data:\n",
    "    test.append(item['output'])\n",
    "\n",
    "    \n",
    "from tqdm import tqdm\n",
    "judgement = {'result': []}\n",
    "model.eval()\n",
    "for row in tqdm(sd):\n",
    "    with torch.no_grad():\n",
    "        if int(row['id'])<185:\n",
    "            continue\n",
    "        instruction = row['instruction']\n",
    "        output = row['output'][:1600]\n",
    "        ori = data[row['id']]['output'][:1600]\n",
    "        messages = [\n",
    "            {\"role\": 'system', \"content\": \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \\\"[[A]]\\\" if assistant A is better, \\\"[[B]]\\\" if assistant B is better, and \\\"[[C]]\\\" for a tie.\"\n",
    "        },\n",
    "           {\"role\": \"user\", \"content\": \n",
    "            '''[User Question]'''+instruction+'''\n",
    "            [The Start of Assistant A's Answer]''' + ori + '''\n",
    "            [The Start of Assistant B's Answer]''' + output + '''\n",
    "'''\n",
    "           }\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').cuda()\n",
    "        result = model.generate(inputs, max_new_tokens = 1024)\n",
    "        res = tokenizer.decode(result[0][inputs.shape[1]:-1], skip_special_tokens=True)\n",
    "        print(res)\n",
    "        judgement['result'].append(res)\n",
    "        gc.collect()  # 调用垃圾回收\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f2eb9-d710-44f7-b924-e1a6f58667be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(judgement)\n",
    "df.to_csv(\"vicuna_alpaca_judgement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8801ef-12aa-4280-8e79-c81988fac6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3152420-5278-456c-b9b3-ae1c1e96c43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"./vicuna_eval0.csv\")\n",
    "# # df2 = pd.read_csv(\"./vicuna_justeval.csv\")\n",
    "# from tqdm import tqdm\n",
    "# # combined_data = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6bc6dd-b012-4595-9b4a-555dcad121ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "1it [00:55, 55.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"helpfulness\": {\n",
      "    \"reason\": \"The response provides a list of famous actors who started their careers on Broadway, directly addressing the user's query and offering relevant information.\",\n",
      "    \"score\": 5\n",
      "  },\n",
      "  \"clarity\": {\n",
      "    \"reason\": \"The response is well-structured, with a clear introduction and a list of actors presented in a coherent and easy-to-follow manner.\",\n",
      "    \"score\": 5\n",
      "  },\n",
      "  \"factuality\": {\n",
      "    \"reason\": \"The information provided appears to be accurate, with correct details about the actors' Broadway debuts. However, without further verification, it's difficult to confirm the accuracy of every single detail.\",\n",
      "    \"score\": 5\n",
      "  },\n",
      "  \"depth\": {\n",
      "    \"reason\": \"The response provides a good number of examples, but the information about each actor is limited to their Broadway debut. Adding more details about their careers or notable roles would increase the depth of the response.\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  \"engagement\": {\n",
      "    \"reason\": \"The response is informative, but it's presented in a straightforward and factual manner, lacking a conversational tone or engaging language that would make it more interesting to read.\",\n",
      "    \"score\": 2\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:53, 56.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"helpfulness\": {\n",
      "    \"reason\": \"The response provides a general overview of how US states got their names, but it doesn't offer specific examples or delve deeper into the topic. It gives a broad idea, but the user may still have questions about individual state names.\",\n",
      "    \"score\": \"3\"\n",
      "  },\n",
      "  \"clarity\": {\n",
      "    \"reason\": \"The response is well-structured and easy to follow. The ideas are presented in a clear and coherent manner, making it easy for the user to understand the general concept.\",\n",
      "    \"score\": \"5\"\n",
      "  },\n",
      "  \"factuality\": {\n",
      "    \"reason\": \"The response doesn't contain any specific factual information that can be verified, but it also doesn't provide any incorrect information. The general statements made are accurate, but lack specificity.\",\n",
      "    \"score\": \"4\"\n",
      "  },\n",
      "  \"depth\": {\n",
      "    \"reason\": \"The response barely scratches the surface of the topic. It provides a broad overview, but doesn't offer any specific examples or in-depth information about individual state names.\",\n",
      "    \"score\": \"2\"\n",
      "  },\n",
      "  \"engagement\": {\n",
      "    \"reason\": \"The response sounds somewhat engaging, but it's written in a fairly formal and generic tone. It doesn't have a conversational feel or attempt to capture the user's interest.\",\n",
      "    \"score\": \"3\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [03:00, 61.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``` \n",
      "{\n",
      "  \"helpfulness\": {\n",
      "    \"reason\": \"The response effectively addresses the user's query by explaining the basic rules of kickball, providing a relevant solution to help the user understand the game and avoid being taken advantage of by their sister and her girlfriends.\",\n",
      "    \"score\": \"5\"\n",
      "  },\n",
      "  \"clarity\": {\n",
      "    \"reason\": \"The response is well-structured, with ideas presented in a clear and coherent manner. The rules of kickball are explained in a step-by-step format, making it easy to follow and understand.\",\n",
      "    \"score\": \"5\"\n",
      "  },\n",
      "  \"factuality\": {\n",
      "    \"reason\": \"The information provided about the rules of kickball appears to be accurate and truthful. However, it's worth noting that some variations of kickball may have slightly different rules, but the response provides a good general overview of the game.\",\n",
      "    \"score\": \"5\"\n",
      "  },\n",
      "  \"depth\": {\n",
      "    \"reason\": \"The response provides a good overview of the basic rules of kickball, but it doesn't delve too deeply into the game's strategy or nuances. The explanation is sufficient for a beginner, but more experienced players may find it lacking in detail.\",\n",
      "    \"score\": \"3\"\n",
      "  },\n",
      "  \"engagement\": {\n",
      "    \"reason\": \"The response is friendly and approachable, with a conversational tone that makes it feel engaging and human-like. The use of phrases like 'I'd be happy to explain' and 'Let me know if you have any other questions' adds to the response's warmth and welcoming nature.\",\n",
      "    \"score\": \"5\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:05, 62.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"helpfulness\": {\n",
      "    \"reason\": \"The response provides a good overview of the music scene in the 1920s, listing various genres and their characteristics. However, it does not provide specific song or artist recommendations, which might have been more directly helpful to the user.\",\n",
      "    \"score\": 4\n",
      "  },\n",
      "  \"clarity\": {\n",
      "    \"reason\": \"The response is well-structured and easy to follow, with each paragraph focusing on a specific genre of music. The language is clear and concise, making it easy for the user to understand the information presented.\",\n",
      "    \"score\": 5\n",
      "  },\n",
      "  \"factuality\": {\n",
      "    \"reason\": \"The information provided appears to be accurate and truthful, with no noticeable factual errors. The response demonstrates a good understanding of the music genres and their characteristics during the 1920s.\",\n",
      "    \"score\": 5\n",
      "  },\n",
      "  \"depth\": {\n",
      "    \"reason\": \"While the response provides a good overview of the music scene in the 1920s, it does not delve deeply into the topic. The information presented is somewhat general and does not offer many specific details or insights.\",\n",
      "    \"score\": 3\n",
      "  },\n",
      "  \"engagement\": {\n",
      "    \"reason\": \"The response is written in a formal and informative tone, which makes it feel somewhat dry and lacking in engagement. The language is not particularly conversational or attention-grabbing, which might make it less engaging for the user.\",\n",
      "    \"score\": 2\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:04, 61.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"helpfulness\": {\n",
      "    \"reason\": \"The response provides a clear step-by-step guide on how to wrap a present neatly, addressing the user's query directly. However, it only presents one common method, which might not cater to different present shapes or sizes.\",\n",
      "    \"score\": 4\n",
      "  },\n",
      "  \"clarity\": {\n",
      "    \"reason\": \"The response is well-structured, with each step logically following the previous one. The language used is simple and easy to understand, making it clear for the user to follow along.\",\n",
      "    \"score\": 5\n",
      "  },\n",
      "  \"factuality\": {\n",
      "    \"reason\": \"The information provided is accurate and true, representing a common technique used for gift wrapping. There are no apparent factual errors or inaccuracies in the response.\",\n",
      "    \"score\": 5\n",
      "  },\n",
      "  \"depth\": {\n",
      "    \"reason\": \"The response provides a basic, general approach to gift wrapping but lacks detail and variety. It does not cover different wrapping techniques, edge folding, or handling various present shapes, making it a relatively superficial guide.\",\n",
      "    \"score\": 2\n",
      "  },\n",
      "  \"engagement\": {\n",
      "    \"reason\": \"The response sounds somewhat robotic and lacks a conversational tone. While it is clear and easy to understand, it does not feel engaging or interactive, reading more like a set of instructions than a helpful guide.\",\n",
      "    \"score\": 2\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "judgement = {'result': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6543b3ba-2c3c-4120-a7e7-35e7a93da5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(judgement)\n",
    "df.to_csv(\"vicuna_judgement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50257ee0-f1bc-4dc5-aad9-32eafc9a41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "270bd22f-898e-4e73-8e65-5893482b11f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': ['```\\n{\\n  \"helpfulness\": {\\n    \"reason\": \"The response provides a list of famous actors who started their careers on Broadway, directly addressing the user\\'s query and offering relevant information.\",\\n    \"score\": 5\\n  },\\n  \"clarity\": {\\n    \"reason\": \"The response is well-structured, with a clear introduction and a list of actors presented in a coherent and easy-to-follow manner.\",\\n    \"score\": 5\\n  },\\n  \"factuality\": {\\n    \"reason\": \"The information provided appears to be accurate, with correct details about the actors\\' Broadway debuts. However, without further verification, it\\'s difficult to confirm the accuracy of each specific detail.\",\\n    \"score\": 5\\n  },\\n  \"depth\": {\\n    \"reason\": \"The response provides a list of 10 actors, but only includes brief information about their Broadway debuts. It does not delve deeper into their careers or provide additional context.\",\\n    \"score\": 3\\n  },\\n  \"engagement\": {\\n    \"reason\": \"The response is informative but lacks a conversational tone, feeling more like a list of facts than a engaging discussion. The language is straightforward and lacks personality.\",\\n    \"score\": 2\\n  }\\n}\\n```',\n",
       "  '```json\\n{\\n  \"helpfulness\": {\\n    \"reason\": \"The response provides some general information about the origins of US state names, but it does not fully address the user\\'s query or provide a comprehensive solution. The answer is too vague and lacks specific examples.\",\\n    \"score\": \"2\"\\n  },\\n  \"clarity\": {\\n    \"reason\": \"The response is structured in a clear and coherent manner, but it lacks a logical flow of ideas. The transition between sentences is abrupt, and the text could be improved with more cohesive linking.\",\\n    \"score\": \"3\"\\n  },\\n  \"factuality\": {\\n    \"reason\": \"The response contains some factual inaccuracies, such as the claim that Texas was named after the Mexican state of Tamaulipas. This is incorrect, as Texas was actually named after the Caddo word \\'tejas,\\' meaning \\'friends.\\'\",\\n    \"score\": \"2\"\\n  },\\n  \"depth\": {\\n    \"reason\": \"The response barely scratches the surface of the topic, providing only a brief overview of the origins of US state names. It lacks specific examples, historical context, and in-depth analysis.\",\\n    \"score\": \"1\"\\n  },\\n  \"engagement\": {\\n    \"reason\": \"The response sounds somewhat robotic and lacks a natural tone. The language is formal, but it does not feel engaging or conversational.\",\\n    \"score\": \"2\"\\n  }\\n}\\n```']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3dea7b-b4a8-4adb-91f1-7e550c0d78fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
